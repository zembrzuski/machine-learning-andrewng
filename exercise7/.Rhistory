rm(list=ls())
hypothesis <- function(x, theta) {
z <- x %*% t(theta)
1/(1+exp(-z))
}
addBiasTherm <- function(x) {
cbind(rep(1, nrow(x)), x)
}
cost <- function(x, y, theta) {
m <- nrow(x)
hipo <- hypothesis(x, theta)
vectorCostForEachPoint <- -y*log(hipo) - (1-y)*log(1-hipo)
1/m * sum(vectorCostForEachPoint)
}
input <- rbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input <- addBiasTherm(input)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
thetasSecondLayer <- rbind(c(10, -20, -20))
outputLayerOne <- hypothesis(input, thetasFirstLayer)
inputLayerTwo <- addBiasTherm(outputLayerOne)
output <- hypothesis(inputLayerTwo, thetasSecondLayer)
output
y
output
ouput > 0.9
output > 0.9
y = rbind(1, 0, 0, 0)
errorLayer4 <- output - y
dim(t(thetasSecondLayer))
dim(errorLayer4)
dim(thetasSecondLayer)
dim(t(thetasSecondLayer))
dim(errorLayer4)
y = rbind(1, 0, 0, 0)
dim(y)
errorLayer4 <- output - y
errorLayer4
dim(errorLayer4)
errorLayer4 <- t(errorLayer4)
errorLayer4
dim(errorLayer4)
input <- rbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input <- addBiasTherm(input)
dim(input)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
thetasSecondLayer <- rbind(c(10, -20, -20))
outputLayerOne <- hypothesis(input, thetasFirstLayer)
inputLayerTwo <- addBiasTherm(outputLayerOne)
inputLayerTwo
dim(inputLayerTwo)
thetasSecondLayer <- rbind(c(10, -20, -20))
dim(thetasSecondLayer)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
dim(thetasFirstLayer)
rm(list=ls())
#rm(list = ls())
hypothesis <- function(x, theta) {
z <- x %*% t(theta)
1/(1+exp(-z))
}
addBiasTherm <- function(x) {
cbind(rep(1, nrow(x)), x)
}
cost <- function(x, y, theta) {
m <- nrow(x)
hipo <- hypothesis(x, theta)
vectorCostForEachPoint <- -y*log(hipo) - (1-y)*log(1-hipo)
1/m * sum(vectorCostForEachPoint)
}
input <- rbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input <- addBiasTherm(input)
dim(input)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
dim(thetasFirstLayer)
thetasSecondLayer <- rbind(c(10, -20, -20))
dim(thetasSecondLayer)
## propagation for the first layer
outputLayerOne <- hypothesis(input, thetasFirstLayer)
## propagation for the second layer
inputLayerTwo <- addBiasTherm(outputLayerOne)
dim(inputLayerTwo)
output <- hypothesis(inputLayerTwo, thetasSecondLayer)
dim(outpu)
dim(output)
rm(list=ls())
source('~/github-machine-learning-coursera/neural-network-recomecando/basefunctions.R')
input <- rbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input <- addBiasTherm(input)
dim(input)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
dim(thetasFirstLayer)
thetasSecondLayer <- rbind(c(10, -20, -20))
dim(thetasSecondLayer)
## propagation for the first layer
outputLayerOne <- hypothesis(input, thetasFirstLayer)
## propagation for the second layer
inputLayerTwo <- addBiasTherm(outputLayerOne)
dim(input)
dim(inputLayerTwo)
dim(output)
output <- hypothesis(inputLayerTwo, thetasSecondLayer)
dim(output)
rm(list=ls())
input <- rbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input <- addBiasTherm(input)
source('~/github-machine-learning-coursera/neural-network-recomecando/basefunctions.R')
input <- addBiasTherm(input)
dim(input)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
dim(thetasFirstLayer)
z <- input %*% t(thetasFirstLayer)
dim(z)
source('~/github-machine-learning-coursera/neural-network-recomecando/basefunctions.R')
rm(list=ls())
input <- cbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input
input <- addBiasTherm(input)
source('~/github-machine-learning-coursera/neural-network-recomecando/basefunctions.R')
input <- addBiasTherm(input)
input
dim(input)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
dim
dim(thetasFirstLayer)
theta1 <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
dim(theta1)
theta2 <- rbind(c(10, -20, -20))
dim(theta2)
a1 = x
a2 = hypothesis(a1, theta1)
a1 = x
a2 = hypothesis(a1, theta1)
a1
a1 = x
a1 = input
a1
a2 = hypothesis(a1, theta1)
source('~/github-machine-learning-coursera/neural-network-recomecando/basefunctions.R')
a2 = hypothesis(a1, theta1)
a2 = hypothesis(theta1, a1)
a2
a2 = addBiasTherm(hypothesis(theta1, a1))
dim(a2)
a2
a3 = addBiasTherm(hypothesis(theta2, a2))
a3
a3 = hypothesis(theta2, a2)
a3
a3 > 0.9
y = cbind(c(1, 0, 0, 0))
y
y = rbind(c(1, 0, 0, 0))
y
output = rbind(c(1, 0, 0, 0))
y = output
error3 = a3 - y
error3 = a3 - y
error3
error2 = t(theta2)
t(theta2)
error2 = t(theta2) %*% error3 * (a2 * (1-a2))
error2
theta1
theta1
for(i in 1:nrow(theta1)) {
for(j in 1:ncol(theta1)) {
print(theta[i, j])
}
}
for(i in 1:nrow(theta1)) {
for(j in 1:ncol(theta1)) {
print(theta1[i, j])
}
}
for(i in 1:nrow(theta1)) {
for(j in 1:ncol(theta1)) {
print(a1[,j] * error2[i, ])
}
}
error2
for(i in 1:nrow(theta1)) {
for(j in 1:ncol(theta1)) {
print(a1[,j] * error2[i, ])
}
}
a1[,1]
error2[1,]
a1[1,]
error2[,1]
a1
error2
a1
a[,2]
a1[,2]
error[2,]
error2[2,]
c(4, 5) * c(2, -1)
c(4, 5) * c(2, -1, 0)
a1
error2
a1[,3]
error2[3,]
error2
a2[,1]
error3[1,]
22790/(22790+57737+7003)
1-(22790/(22790+57737+7003))
0.915^2
1-0.12
0.88^4
1-0.88^4
24/112
24/625
100-7
0.93/0.05
0.07/0.05
0.03/0.95
0.97/0.95
0.93*0.05
0.07*0.05
0.03*0.95
0.97*0.95
0.9215+0.0285+0.0035+0.0465
0.0465/(0.0465+0.0285)
24/46
rm(list = ls())
n <- 150 # number of data points
p <- 2 # dimension
sigma <- 1 # variance of the distribution
meanpos <- 0 # centre of the distribution of positive examples
meanneg <- 3 # centre of the distribution of negative examples
npos <- round(n/2) # number of positive examples
nneg <- n-npos # number of negative examples
# Generate the positive and negative examples
xpos <- matrix(rnorm(npos*p,mean=meanpos,sd=sigma),npos,p)
xneg <- matrix(rnorm(nneg*p,mean=meanneg,sd=sigma),npos,p)
x <- rbind(xpos,xneg)
# Generate the labels
y <- matrix(c(rep(1,npos),rep(-1,nneg)))
# Visualize the data
plot(x,col=ifelse(y>0,1,2))
legend("topleft",c(’Positive’,’Negative’),col=seq(2),pch=1,text.col=seq(2))
ntrain <- round(n*0.8) # number of training examples
tindex <- sample(n,ntrain) # indices of training samples
xtrain <- x[tindex,]
xtest <- x[-tindex,]
ytrain <- y[tindex]
ytest <- y[-tindex]
istrain=rep(0,n)
istrain[tindex]=1
plot(x,col=ifelse(y>0,1,2),pch=ifelse(istrain==1,1,2))
ntrain <- round(n*0.8) # number of training examples
tindex <- sample(n,ntrain) # indices of training samples
xtrain <- x[tindex,]
xtest <- x[-tindex,]
ytrain <- y[tindex]
ytest <- y[-tindex]
istrain=rep(0,n)
istrain[tindex]=1
# Visualize
plot(x,col=ifelse(y>0,1,2),pch=ifelse(istrain==1,1,2))
legend("topleft",c(’Positive Train’,’Positive Test’,’Negative Train’,’Negative Test’),
col=c(1,1,2,2),pch=c(1,2,1,2),text.col=c(1,1,2,2))
library(kernlab)
install.packages(kernlab)
library(kernlab)
install.packages('kernlab')
library(kernlab)
library(kernlab)
install.packages('kernlab')
install.packages('kernlab')
install.packages('kernlab')
library(kernlab)
xtrain
ytrain
svp <- ksvm(xtrain, ytrain, type='C-svc', kernel='vanilladot', C=100, scaled=c())
svp
attributes(svp)
alpha(svp)
plot(svp, data=xtrain)
svp
attributes(svp)
alpha(svp)
alphaindex(svp)
b(svp)
svp
attributes(svp)
alpha(svp)
alphaindex(svp)
b(svp)
alpha(svp)
alphaindex(svp)
plot(svp, data=xtrain)
b(svp)
svp
attributes(svp)
alpha(svp)
?alpha
?alpha
?alphaindex
?alphaindex
alphaindex(svp)
60043+4000+1800
60043+1800+4000+1000
60043+1800+4000+1000+2500
rm(list = ls())
setwd('/home/nozes/github-machine-learning-coursera/exercise7')
X <- as.matrix(read.csv('input/data1.txt', header=FALSE, sep=' '))
plot(X)
X
mean(X)
mean(X[,1])
mean(X[,2])
X[,1] - mean(X[,1])
mean(X[,1] - mean(X[,1]))
mean(X[,2] - mean(X[,2]))
mean(X[,2] - mean(X[,2])) < 0.0001
mean(X[,1] - mean(X[,1])) > -0.0001
centeredColumn1 <- mean(X[,1] - mean(X[,1]))
centeredColumn2 <- mean(X[,2] - mean(X[,2]))
max(centeredColumn1)
normalizedColumn1 <- min(centeredColumn1)
normalizedColumn1 <- centeredColumn1/(max(centeredColumn1)-min(centeredColumn1))
normalizedColumn2 <- centeredColumn2/(max(centeredColumn2)-min(centeredColumn2))
plot(X)
plot(cbind(normalizedColumn1, normalizedColumn2))
x <- X[,1]
normalized = (x-min(x))/(max(x)-min(x))
x <- X[,1]
normalized1 = (x-min(x))/(max(x)-min(x))
normalized2 = (x-min(x))/(max(x)-min(x))
plot(cbind(normalized1, normalized2))
cbind(normalized1, normalized2)
plot(X)
plot(cbind(normalized1, normalized2))
rm(list = ls())
setwd('/home/nozes/github-machine-learning-coursera/exercise7')
X <- as.matrix(read.csv('input/data1.txt', header=FALSE, sep=' '))
plot(X)
centeredColumn1 <- X[,1] - mean(X[,1]))
centeredColumn1 <- X[,1] - mean(X[,1])
centeredColumn2 <- X[,2] - mean(X[,2]))
plot(cbind(centererdColumn1, centeredColumn2))
plot(cbind(centererdColumn1, centeredColumn2))
plot(cbind(centeredColumn1, centeredColumn2))
centeredColumn1
centeredColumn2
centeredColumn2 <- X[,2] - mean(X[,2]))
plot(cbind(centeredColumn1, centeredColumn2))
centeredColumn1 <- X[,1] - mean(X[,1])
centeredColumn2 <- X[,2] - mean(X[,2]))
plot(cbind(centeredColumn1, centeredColumn2))
centeredColumn2 <- X[,2] - mean(X[,2]))
centeredColumn2 <- X[,2] - mean(X[,2])
plot(cbind(centeredColumn1, centeredColumn2))
normalizedColumn1 <- centeredColumn1/(max(centeredColumn1)-min(centeredColumn1))
normalizedColumn2 <- centeredColumn2/(max(centeredColumn2)-min(centeredColumn2))
plot(cbind(normalizedColumn1, normalizedColumn2))
plot(X)
plot(cbind(normalizedColumn1, normalizedColumn2))
normalizedX <- cbind(normalizedColumn1, normalizedColumn2)
plot(normalizedX)
dim(normalizedX)
dim(t(normalizedx))
dim(t(normalizedX))
sigma <- 1/nrow(normalizedX) * t(normalizedX) %*% normalizedX
sigma
svd(sigma)
svd(sigma)$s
svd(sigma)$v
svd(sigma)$u
svd(sigma)$s
svd(sigma)$S
svd(sigma)
dim(normalizedX)
svd(sigma)$u
ureduce <- svd(sigma)$u[,1]
ureduce
ureduce <- cbind(ureduce)
ureduce
dim(t(ureduce))
dim(x)
dim(normalizedX)
ureduce <- cbind(ureduce)
dim(t(ureduce))
dim(ureduce)
dim(ureduce)
dim(t(ureduce))
dim(normalizedX)
cbind(c(1, 2), c(3, 4), c(5, 6))
cbind(c(1, 2), c(3, 4), c(5, 6)).svd
foo <- cbind(c(1, 2), c(3, 4), c(5, 6))
foo.svd()
foo <- cbind(c(1, 2), c(3, 4), c(5, 6))
svd(foo)
svd(foo)$u
foo
svd(foo)$u
ureduce <- svd(sigma)$u[,1]
ureduce <- cbind(ureduce)
ureduce
dim(t(ureduce))
dim(normalizedX)
dim(t(normalizedX))
dim(t(ureduce))
dim(t(normalizedX))
t(ureduce) %*% t(normalizedX)
(t(ureduce) %*% t(normalizedX))[1,]
z <- (t(ureduce) %*% t(normalizedX))[1,]
z
dim(ureduce)
cbind(z)
z <- cbind(z)
z
z <- rbind(z)
z <- (t(ureduce) %*% t(normalizedX))[1,]
z <- rbind(z)
z
dim(z)
dim(ureduce)
ureduce %*% z
xapprox <- ureduce %*% z
dim(xapprox)
xapprox <- t(xapprox)
dim(xapprox)
xapprox
plot(xapprox)
plot(normalizedX)
plot(xapprox)
rm(list = ls())
setwd('/home/nozes/github-machine-learning-coursera/exercise7')
source('pca_funcions.R')
X <- as.matrix(read.csv('input/data1.txt', header=FALSE, sep=' '))
plot(X)
foo <- matrix(NA, col=2, row=2)
foo <- matrix(NA, ncol=2, nrow=2)
foo
source('~/github-machine-learning-coursera/exercise7/pca_funcions.R')
X <- as.matrix(read.csv('input/data1.txt', header=FALSE, sep=' '))
plot(X)
plot(normalizeFeatures(X))
plot(X)
plot(normalizeFeatures(X))
source('~/github-machine-learning-coursera/exercise7/pca_funcions.R')
source('~/github-machine-learning-coursera/exercise7/pca_funcions.R')
normalizedX <- normalizeFeatures(X)
calculateUreduce(normalizedX, 1)
ureduce <- calculateUreduce(normalizedX, 1)
calculateZ(ureduce, normalizedX)
z <- calculateZ(ureduce, normalizedX)
dim(z)
source('~/github-machine-learning-coursera/exercise7/pca_funcions.R')
reconstructMatrix(ureduce, z)
plot(reconstructMatrix(ureduce, z))
svd(X)
?svd
rbind(c(1, 2, 3), c(4, 5, 6))
svd(rbind(c(1, 2, 3), c(4, 5, 6)))
