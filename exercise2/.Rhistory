plot(input$V1, input$V2, col = input$V3)
plot(input$V1, input$V2)
head(input)
plot(input$V1, input$V2, col = input$V3)
plot(input$V1, input$V2, col = input$V3)
head(input)
input2 <- read.csv2('/home/nozes/Desktop/ml2/ex2/ex2data1.txt' , header = FALSE, sep = ',', stringsAsFactors = TRUE)
input$V3 <- input2$V3
plot(input$V1, input$V2, col = input$V3)
input <- read.csv2('/home/nozes/Desktop/ml2/ex2/ex2data1.txt' , header = FALSE, sep = ',', stringsAsFactors = FALSE)
input2 <- read.csv2('/home/nozes/Desktop/ml2/ex2/ex2data1.txt' , header = FALSE, sep = ',', stringsAsFactors = TRUE)
input$V3 <- input2$V3
plot(input$V1, input$V2, col = input$V3)
rm(list = ls())
input <- read.csv2('/home/nozes/Desktop/ml2/ex2/ex2data1.txt' , header = FALSE, sep = ',', stringsAsFactors = FALSE)
input
input$V3 == 1
sapply(input$V3, function(x){
if (x == 0) {
return("red")
} else {
return("blue")
}
})
colors <- sapply(input$V3, function(x){
if (x == 0) {
return("red")
} else {
return("blue")
}
})
input$V3 <- colors
plot(input$V1, input$V2, col = input$V3)
plot(input$V1, input$V2, col = input$V3, pch=16)
plot(input$V1, input$V2, col = input$V3, pch=16, cex=0.4)
plot(input$V1, input$V2, col = input$V3, pch=16, cex=0.6)
plot(input$V1, input$V2, col = input$V3, pch=16, cex=0.8)
plot(input$V1, input$V2, col = input$V3, pch=16)
source('~/cousera-ml/logistic-regression/logisticRegression.R')
head(input)
xMatrix <- cbind(input$V1, input$V2)
xMatrix
myData <- cbind(input$V1, input$V2)
myMatrix <- cbind(rep(1, length(nrow(myData))), myData)
myMatrix
myData <- rbind(c(1, 1), c(1, 2), c(2, 1), c(2.5, 0.2), c(2, 2), c(2, 3), c(3, 2))
plot(myData)
myMatrix <- cbind(rep(1, length(nrow(myData))), myData)
myMatrix
head(input)
myData <- cbind(as.numeric(input$V1), input$V2)
myData
myData <- cbind(input$V1, input$V2)
myData
myMatrix <- cbind(rep(1, length(nrow(myData))), myData)
myMatrix
yVector <-- input$V3
yVector <- input$V3
yVector
input <- read.csv2('/home/nozes/Desktop/ml2/ex2/ex2data1.txt' , header = FALSE, sep = ',', stringsAsFactors = FALSE)
yVector <- input$V3
yVector
myMatrix
costs <- gradient(c(0.5, 0.5, 0.5), 0.01, 10000, myMatrix, yVector)
yVector
myMatrix
input$V2
input$V2
input <- read.csv2('/home/nozes/Desktop/ml2/ex2/ex2data1.txt' , header = FALSE, sep = ',', stringsAsFactors = FALSE)
input
myData <- cbind(input$V1, input$V2)
myMatrix <- cbind(rep(1, length(nrow(myData))), myData)
myMatrix
rm(list = ls())
input <- read.csv2('/home/nozes/Desktop/ml2/ex2/ex2data1.txt' , header = FALSE, sep = ',', stringsAsFactors = FALSE)
colors <- sapply(input$V3, function(x){
if (x == 0) {
return("red")
} else {
return("blue")
}
})
input
input$V3 <- colors
input
plot(input$V1, input$V2, col = input$V3, pch=16)
matrix(input$V1, input$V2)
matrix(c(input$V1, input$V2)
matrix(c(input$V1, input$V2))
matrix(c(input$V1, input$V2))
myData <- cbind(input$V1, input$V2)
myData
input
myData <- cbind(input$V1, input$V2)
myData <- cbind(input$V1, input$V2))
myData <- cbind(input$V1, input$V2))
myData <- cbind(input$V1, input$V2)
myData
input$V1
input$V1
input$V1
rm(list = ls())
input <- read.csv2('/home/nozes/Desktop/ml2/ex2/ex2data1.txt' , header = FALSE, sep = ',', stringsAsFactors = FALSE)
colors <- sapply(input$V3, function(x){
if (x == 0) {
return("red")
} else {
return("blue")
}
})
input$V3 <- colors
input
input$V1
as.numeric(input$V1)
myData <- cbind(as.numeric(input$V1), as.numeric(input$V2))
myData
myMatrix <- cbind(rep(1, length(nrow(myData))), myData)
yVector <- input$V3
myMatrix
yVector
input <- read.csv2('/home/nozes/Desktop/ml2/ex2/ex2data1.txt' , header = FALSE, sep = ',', stringsAsFactors = FALSE)
myMatrix
yVector <- input$V3
myMatrix
yVector
costs <- gradient(c(0.5, 0.5, 0.5), 0.01, 10000, myMatrix, yVector)
source('~/cousera-ml/logistic-regression/logisticRegression.R')
costs <- gradient(c(0.5, 0.5, 0.5), 0.01, 10000, myMatrix, yVector)
costs
plot(costs)
costs <- gradient(c(0.5, 0.5, 0.5), 0.01, 10000, myMatrix, yVector)
costs <- gradient(c(0.5, 0.5, 0.5), 0.0.1, 10000, myMatrix, yVector)
costs <- gradient(c(0.5, 0.5, 0.5), 0.001, 10000, myMatrix, yVector)
plot(costs)
costs <- gradient(c(0.5, 0.5, 0.5), 0.0001, 10000, myMatrix, yVector)
plot(costs)
costs <- gradient(c(0.5, 0.5, 0.5), 0.00001, 10000, myMatrix, yVector)
plot(costs)
costs <- gradient(c(0.5, 0.5, 0.5), 0.00005, 10000, myMatrix, yVector)
costs <- gradient(c(0.5, 0.5, 0.5), 0.0001, 10000, myMatrix, yVector)
plot(costs)
costs <- gradient(c(0.5, 0.5, 0.5), 0.00009, 10000, myMatrix, yVector)
plot(costs)
costs <- gradient(c(0.5, 0.5, 0.5), 0.00008, 10000, myMatrix, yVector)
plot(costs)
costs <- gradient(c(0.5, 0.5, 0.5), 0.00007, 10000, myMatrix, yVector)
plot(costs)
costs <- gradient(c(0.5, 0.5, 0.5), 0.00006, 10000, myMatrix, yVector)
plot(costs)
costs <- gradient(c(0.5, 0.5, 0.5), 0.00005, 10000, myMatrix, yVector)
plot(costs)
costs <- gradient(c(0.5, 0.5, 0.5), 0.00002, 10000, myMatrix, yVector)
plot(costs)
costs <- gradient(c(0.5, 0.5, 0.5), 0.00001, 10000, myMatrix, yVector)
plot(costs)
costs <- gradient(c(0.5, 0.5, 0.5), 0.00001, 50000, myMatrix, yVector)
plot(costs)
costs <- gradient(c(0.5, 0.5, 0.5), 0.000015, 20000, myMatrix, yVector)
plot(costs)
costs <- gradient(c(0.5, 0.5, 0.5), 0.00001, 50000, myMatrix, yVector)
plot(costs)
costs <- gradient(c(0.5, 0.5, 0.5), 0.00001, 100000, myMatrix, yVector)
plot(costs)
costs <- gradient(c(0.5, 0.5, 0.5), 0.00001, 200000, myMatrix, yVector)
costs <- gradient(c(0.5, 0.5, 0.5), 0.00001, 200000, myMatrix, yVector)
costs <- gradient(c(0.5, 0.5, 0.5), 0.00001, 100000, myMatrix, yVector)
plot(costs)
predicting <- hypothesis(c(-4.578479, 0.04352324, 0.03635591), myMatrix) >= 0.5
predicting <- hypothesis(c(-4.578479, 0.04352324, 0.03635591), myMatrix) >= 0.5
rbind(predicting, yVector)
cbind(predicting, yVector)
predicting <- hypothesis(c(-4.578479, 0.04352324, 0.03635591), myMatrix) >= 0.5
rbind(predicting, yVector)
plot(input$V1, input$V2, col = input$V3, pch=16)
input$V3 <- colors
plot(input$V1, input$V2, col = input$V3, pch=16)
hypothesis(c(-4.578479, 0.04352324, 0.03635591), c(1,70,30)) >= 0.5
hypothesis(c(-4.578479, 0.04352324, 0.03635591), cbind((c1,70,30))) >= 0.5
hypothesis(c(-4.578479, 0.04352324, 0.03635591), cbind(c(1,70,30))) >= 0.5
hypothesis(c(-4.578479, 0.04352324, 0.03635591), c(1,30,40)) >= 0.5
cbind(c(1,30,40))
rbind(c(1,30,40))
as.matrix(rbind(c(1,30,40)))
rrr <- as.matrix(rbind(c(1,30,40)))
hypothesis(c(-4.578479, 0.04352324, 0.03635591), rrr) >= 0.5
rrr <- as.matrix(rbind(c(1,100,80)))
hypothesis(c(-4.578479, 0.04352324, 0.03635591), rrr) >= 0.5
rrr <- as.matrix(rbind(c(1,70,50)))
hypothesis(c(-4.578479, 0.04352324, 0.03635591), rrr) >= 0.5
rrr <- as.matrix(rbind(c(1,70,40)))
hypothesis(c(-4.578479, 0.04352324, 0.03635591), rrr) >= 0.5
rrr <- as.matrix(rbind(c(1,70,40)))
hypothesis(c(-4.578479, 0.04352324, 0.03635591), rrr)
rrr <- as.matrix(rbind(c(1,70,45)))
hypothesis(c(-4.578479, 0.04352324, 0.03635591), rrr)
rrr <- as.matrix(rbind(c(1,45,485)))
hypothesis(c(-4.578479, 0.04352324, 0.03635591), rrr)
rrr <- as.matrix(rbind(c(1,45,85)))
hypothesis(c(-4.578479, 0.04352324, 0.03635591), rrr)
costs <- gradient(c(0.5, 0.5, 0.5), 0.00001, 150000, myMatrix, yVector)
rrr <- as.matrix(rbind(c(1,45,85)))
hypothesis(c(-6.099932, 0.05508169, 0.04833282), rrr)
input <- read.csv2('/home/nozes/Desktop/ml2/ex2/ex2data2.txt' , header = FALSE, sep = ',', stringsAsFactors = FALSE)
input
colors <- sapply(input$V3, function(x){
if (x == 0) {
return("red")
} else {
return("blue")
}
})
input$V3 <- colors
plot(input$V1, input$V2, col = input$V3, pch=16)
rm(list = ls())
source("http://bit.ly/dasi_inference")
rm(list = ls())
source("http://bit.ly/dasi_inference")
load(url("http://www.openintro.org/stat/data/atheism.RData"))
head(atheism)
subset(atheism, nationality == 'United States')
head(atheism)
subset(atheism, nationality == 'United States' & year == '2012')
us12 <- subset(atheism, nationality == 'United States' & year == '2012')
head(us12)
us12$response
subset(us12, response == 'atheist')
nrow(subset(us12, response == 'atheist'))
nrow(us12)
50/1002
pnorm(0.975)
sqrt(0.0499002*(1-0.0499002)/1002)
pnorm(0.975)
pnorm(0.975) * sqrt(0.0499002*(1-0.0499002)/1002)
inference(us12$response, est = "proportion", type = "ci", method = "theoretical", success = "atheist")
(0.0634 - 0.0364)/2
p <- 50/1002
n <- nrow(us12)
se <- sqrt( p*(1-p)/n )
se
qnorm(0.975)
qnorm(0.975) * se
n <- 1000
p <- seq(0, 1, 0.01)
me <- 2*sqrt(p*(1 - p)/n)
plot(me ~ p)
subset(atheism, nationality == 'Spain' & year == '2012')
load(url("http://www.openintro.org/stat/data/atheism.RData"))
subset(atheism, nationality == 'Spain' & year == '2012')
head(subset(atheism, nationality == 'Spain' & year == '2012'))
subset(atheism, nationality == 'Spain' & year == '2012')$year
subset(atheism, nationality == 'Spain')$year
spain12 <- subset(atheism, nationality == 'Spain' & year == '2012')
spain05 <- subset(atheism, nationality == 'Spain' & year == '2005')
spain12
spain05
head(spain12)
subset(spain12, response == 'atheist')
nrow(subset(spain12, response == 'atheist'))
nrow(spain12)
103/1145
nrow(subset(spain05, response == 'atheist'))
nrow(spain05)
115/1146
qnorm(p = 0.975)
(0.0899-0.1003) + 1.96*sqrt(  (0.0899*(1-0899)/1145)    +   (0.1003*(1-0.1003)/1146)  )
(0.0899-0.1003) + 1.96*sqrt(  (0.0899*(1-0.0899)/1145)    +   (0.1003*(1-0.1003)/1146)  )
(0.0899-0.1003) - 1.96*sqrt(  (0.0899*(1-0.0899)/1145)    +   (0.1003*(1-0.1003)/1146)  )
spain12 <- subset(atheism, nationality == 'United States' & year == '2012')
spain05 <- subset(atheism, nationality == 'United States' & year == '2005')
spain12
spain05
spain12 <- subset(atheism, nationality == 'United States' & year == '2012')
spain05 <- subset(atheism, nationality == 'United States' & year == '2005')
spain12
us12 <- subset(atheism, nationality == 'United States' & year == '2012')
us05 <- subset(atheism, nationality == 'United States' & year == '2005')
nrow(us05)
nrow(subset(us05, response == 'atheist'))
nrow(us12)
us12 <- subset(atheism, nationality == 'United States' & year == '2012')
nrow(us12)
nrow(subset(us12, response == 'atheist'))
10/1002
50/1002
(0.0499-0.0099) + 1.96 * sqrt(  (0.0499*(1-0.0499)/1002)    + (0.0099*(1-0.0099)/1002)   )
(0.0499-0.0099) - 1.96 * sqrt(  (0.0499*(1-0.0499)/1002)    + (0.0099*(1-0.0099)/1002)   )
n <- 1000
p <- seq(0, 1, 0.01)
me <- 2*sqrt(p*(1 - p)/n)
plot(me ~ p)
n <- 10
me <- 2*sqrt(0.5*(1 - 0.5)/n)
n <- 10
2*sqrt(0.5*(1 - 0.5)/n)
n <- 100
2*sqrt(0.5*(1 - 0.5)/n)
n <- 9604
2*sqrt(0.5*(1 - 0.5)/n)
n <- 2401
2*sqrt(0.5*(1 - 0.5)/n)
n <- 9602
me <- 2*sqrt(0.5*(1 - 0.5)/n)
n <- 9604
me <- 2*sqrt(0.5*(1 - 0.5)/n)
print(me)
p1 <- 493/1037
n1 <- 1037
p2 <- 596/1028
n2 <- 1028
sqrt(p1*(1-p1)/n1 + p2*(1-p2)/n2)
qnorm(0.975)
qt(0.975, 25)
rm(list=ls())
source('~/github-machine-learning-coursera/exercise2/function-definitions2_2.R')
setwd('/home/nozes/github-machine-learning-coursera/exercise2')
data <- read.csv2('input/ex2data2.txt', sep = ',' , header = FALSE, stringsAsFactors = FALSE)
plot(data$V1, data$V2, col = data$V3)
x <- cbind(as.numeric(as.character(data$V1)), as.numeric(as.character(data$V2)))
x <- cbind(rep(1, nrow(x)), x)
y <- as.numeric(as.character(data$V3))
length(y)
x <- mapFeature(x)
nrow(x)
length(y)
theta <- rep(0, 28)
lambda <- 1
print(cost(x, y, theta, lambda))
xxx <- derivativeTheta(x, y, theta, 0.05, 40000, lambda)
plot(xxx)
prediction <- (hypothesis(thetas, x)[1,] > 0.5)
realAnswer <- (y > 0.5)
comparison <- prediction == realAnswer
thetas <- c(
1.27273951,  0.62527180,  1.18108869, -2.01996086, -0.91742375, -1.43166444,  0.12400635, -0.36553437, -0.35723962, -0.17513048, -1.45815646,
-0.05098906, -0.61555504, -0.27470631, -1.19281652, -0.24218823, -0.20600609, -0.04473075, -0.27778450, -0.29537810, -0.45635749, -1.04320249,
0.02777171, -0.29243131,  0.01556680, -0.32737959, -0.14388693, -0.92465257
)
thetas
prediction <- (hypothesis(thetas, x)[1,] > 0.5)
realAnswer <- (y > 0.5)
comparison <- prediction == realAnswer
accuracy <- length(comparison[comparison == TRUE]) / length(comparison)
paste('accuracy: ', accuracy)
source('~/github-machine-learning-coursera/exercise2/function-definitions2_2.R')
rm(list=ls())
source('~/github-machine-learning-coursera/exercise2/function-definitions2_2.R')
setwd('/home/nozes/github-machine-learning-coursera/exercise2')
data <- read.csv2('input/ex2data2.txt', sep = ',' , header = FALSE, stringsAsFactors = FALSE)
plot(data$V1, data$V2, col = data$V3)
# extracting x axis from the dataset
x <- cbind(as.numeric(as.character(data$V1)), as.numeric(as.character(data$V2)))
# appending 1 column (bias)
x <- cbind(rep(1, nrow(x)), x)
# extracting y from the dataset
y <- as.numeric(as.character(data$V3))
length(y)
x <- mapFeature(x)
nrow(x)
length(y)
theta <- rep(0, 28)
lambda <- 1
print(cost(x, y, theta, lambda))
xxx <- derivativeTheta(x, y, theta, 0.05, 40000, lambda)
plot(xxx)
source('~/github-machine-learning-coursera/exercise2/function-definitions2_2.R')
source('~/github-machine-learning-coursera/exercise2/function-definitions2_2.R')
source('~/github-machine-learning-coursera/exercise2/function-definitions2_2.R')
source('~/github-machine-learning-coursera/exercise2/function-definitions2_2.R')
source('~/github-machine-learning-coursera/exercise2/function-definitions2_2.R')
source('~/github-machine-learning-coursera/exercise2/function-definitions2_2.R')
rm(list=ls())
source('~/github-machine-learning-coursera/exercise2/function-definitions2_2.R')
setwd('/home/nozes/github-machine-learning-coursera/exercise2')
data <- read.csv2('input/ex2data2.txt', sep = ',' , header = FALSE, stringsAsFactors = FALSE)
plot(data$V1, data$V2, col = data$V3)
# extracting x axis from the dataset
x <- cbind(as.numeric(as.character(data$V1)), as.numeric(as.character(data$V2)))
# appending 1 column (bias)
x <- cbind(rep(1, nrow(x)), x)
# extracting y from the dataset
y <- as.numeric(as.character(data$V3))
length(y)
x <- mapFeature(x)
theta <- rep(0, 28)
lambda <- 1
print(cost(x, y, theta, lambda))
costs <- gradientDescent(x, y, theta, 0.05, 40000, lambda)
plot(costs)
rm(list=ls())
source('~/github-machine-learning-coursera/exercise2/function-definitions2_2.R')
setwd('/home/nozes/github-machine-learning-coursera/exercise2')
data <- read.csv2('input/ex2data2.txt', sep = ',' , header = FALSE, stringsAsFactors = FALSE)
plot(data$V1, data$V2, col = data$V3)
# extracting x axis from the dataset
x <- cbind(as.numeric(as.character(data$V1)), as.numeric(as.character(data$V2)))
# appending 1 column (bias)
x <- cbind(rep(1, nrow(x)), x)
# extracting y from the dataset
y <- as.numeric(as.character(data$V3))
length(y)
x <- mapFeature(x)
theta <- rep(0, 28)
lambda <- 0.3
print(cost(x, y, theta, lambda))
costs <- gradientDescent(x, y, theta, 0.05, 40000, lambda)
plot(costs)
thetasAnswer <- c(
thetasAnswer <- c(
2.03549856,  1.25050764,  2.06561176, -3.22801315, -2.01870134, -2.72457278,  0.39785991, -0.74196296, -0.58275100, -0.23188667, -2.38396961,
0.05676676, -1.13841819, -0.67085863, -1.97743415, -0.36980196, -0.36452072,  0.09074282, -0.59678448, -0.66181802, -0.43599292, -1.76047665,
0.15624171, -0.51533607,  0.11217032, -0.64058461, -0.42692479, -1.23625525
)
prediction <- (hypothesis(thetasAnswer, x)[1,] > 0.5)
thetasAnswer <- c(
2.03549856,  1.25050764,  2.06561176, -3.22801315, -2.01870134, -2.72457278,  0.39785991, -0.74196296, -0.58275100, -0.23188667, -2.38396961,
0.05676676, -1.13841819, -0.67085863, -1.97743415, -0.36980196, -0.36452072,  0.09074282, -0.59678448, -0.66181802, -0.43599292, -1.76047665,
0.15624171, -0.51533607,  0.11217032, -0.64058461, -0.42692479, -1.23625525
)
prediction <- (hypothesis(thetasAnswer, x)[1,] > 0.5)
realAnswer <- (y > 0.5)
comparison <- prediction == realAnswer
accuracy <- length(comparison[comparison == TRUE]) / length(comparison)
paste('accuracy: ', accuracy)
thetasAnswer
thetasAnswer <- c(
1.27273951,  0.62527180,  1.18108869, -2.01996086, -0.91742375, -1.43166444,  0.12400635, -0.36553437, -0.35723962, -0.17513048, -1.45815646,
-0.05098906, -0.61555504, -0.27470631, -1.19281652, -0.24218823, -0.20600609, -0.04473075, -0.27778450, -0.29537810, -0.45635749, -1.04320249,
0.02777171, -0.29243131,  0.01556680, -0.32737959, -0.14388693, -0.92465257
)
prediction <- (hypothesis(thetasAnswer, x)[1,] > 0.5)
realAnswer <- (y > 0.5)
comparison <- prediction == realAnswer
accuracy <- length(comparison[comparison == TRUE]) / length(comparison)
paste('accuracy: ', accuracy)
thetasAnswer <- c(
2.03549856,  1.25050764,  2.06561176, -3.22801315, -2.01870134, -2.72457278,  0.39785991, -0.74196296, -0.58275100, -0.23188667, -2.38396961,
0.05676676, -1.13841819, -0.67085863, -1.97743415, -0.36980196, -0.36452072,  0.09074282, -0.59678448, -0.66181802, -0.43599292, -1.76047665,
0.15624171, -0.51533607,  0.11217032, -0.64058461, -0.42692479, -1.23625525
)
prediction <- (hypothesis(thetasAnswer, x)[1,] > 0.5)
realAnswer <- (y > 0.5)
comparison <- prediction == realAnswer
accuracy <- length(comparison[comparison == TRUE]) / length(comparison)
paste('accuracy: ', accuracy)
lambda
lambda <- 0
print(cost(x, y, theta, lambda))
costs <- gradientDescent(x, y, theta, 0.05, 40000, lambda)
plot(costs)
thetasAnswer <- c(
3.7505816,  2.5947539,  4.1521981, -5.5027114, -5.1669854, -6.4958353,  1.2616133, -1.5746461, -0.3519664, -0.7791326, -4.5863980,  1.0250135,
-2.6290521, -2.1003556, -3.8011352, -0.9084167, -0.6456020,  1.2450133, -1.8701145, -1.9876461,  0.4372117, -3.9553141,  0.7635819, -1.0668591,
0.7031927, -1.7889541, -1.8082621, -0.9708463
)
prediction <- (hypothesis(thetasAnswer, x)[1,] > 0.5)
realAnswer <- (y > 0.5)
comparison <- prediction == realAnswer
accuracy <- length(comparison[comparison == TRUE]) / length(comparison)
paste('accuracy: ', accuracy)
thetasAnswer <- c(
2.03549856,  1.25050764,  2.06561176, -3.22801315, -2.01870134, -2.72457278,  0.39785991, -0.74196296, -0.58275100, -0.23188667, -2.38396961,
0.05676676, -1.13841819, -0.67085863, -1.97743415, -0.36980196, -0.36452072,  0.09074282, -0.59678448, -0.66181802, -0.43599292, -1.76047665,
0.15624171, -0.51533607,  0.11217032, -0.64058461, -0.42692479, -1.23625525
)
prediction <- (hypothesis(thetasAnswer, x)[1,] > 0.5)
realAnswer <- (y > 0.5)
comparison <- prediction == realAnswer
accuracy <- length(comparison[comparison == TRUE]) / length(comparison)
paste('accuracy: ', accuracy)
thetasAnswer <- c(
1.27273951,  0.62527180,  1.18108869, -2.01996086, -0.91742375, -1.43166444,  0.12400635, -0.36553437, -0.35723962, -0.17513048, -1.45815646,
-0.05098906, -0.61555504, -0.27470631, -1.19281652, -0.24218823, -0.20600609, -0.04473075, -0.27778450, -0.29537810, -0.45635749, -1.04320249,
0.02777171, -0.29243131,  0.01556680, -0.32737959, -0.14388693, -0.92465257
)
prediction <- (hypothesis(thetasAnswer, x)[1,] > 0.5)
realAnswer <- (y > 0.5)
comparison <- prediction == realAnswer
accuracy <- length(comparison[comparison == TRUE]) / length(comparison)
paste('accuracy: ', accuracy)
source('~/github-machine-learning-coursera/exercise2/function-definitions2_2.R')
rm(list=ls())
setwd('/home/nozes/github-machine-learning-coursera/exercise2')
source('~/github-machine-learning-coursera/exercise2/function-definitions2_2.R')
setwd('/home/nozes/github-machine-learning-coursera/exercise2')
data <- read.csv2('input/ex2data2.txt', sep = ',' , header = FALSE, stringsAsFactors = FALSE)
plot(data$V1, data$V2, col = data$V3)
# extracting x axis from the dataset
x <- cbind(as.numeric(as.character(data$V1)), as.numeric(as.character(data$V2)))
# appending 1 column (bias)
x <- cbind(rep(1, nrow(x)), x)
# extracting y from the dataset
y <- as.numeric(as.character(data$V3))
length(y)
x <- mapFeature(x)
theta <- rep(0, 28)
lambda <- 100
print(cost(x, y, theta, lambda))
costs <- gradientDescent(x, y, theta, 0.05, 40000, lambda)
plot(costs)
thetasAnswer <- c(
)
thetasAnswer <- c(
0.0218777139, -0.0174817150,  0.0057107919, -0.0551689468, -0.0131487658, -0.0385985779, -0.0184635583, -0.0077321883, -0.0089242928,
-0.0228045236, -0.0434384618, -0.0023562328, -0.0141561192, -0.0034950765, -0.0414358756, -0.0210059340, -0.0047191663, -0.0035913105,
-0.0063222560, -0.0050244051, -0.0319767563, -0.0341633472, -0.0010762879, -0.0070261508, -0.0003850633, -0.0079822974, -0.0015477887,
-0.0410867689
)
prediction <- (hypothesis(thetasAnswer, x)[1,] > 0.5)
realAnswer <- (y > 0.5)
comparison <- prediction == realAnswer
accuracy <- length(comparison[comparison == TRUE]) / length(comparison)
paste('accuracy: ', accuracy)
