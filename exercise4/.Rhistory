y
y
errorLayer3 <- output - y
errorLayer3
thetasFirstLayer
errorLayer2 <- t(thetasFirstLayer)
t(thetasFirstLayer)
dim(t(thetasFirstLayer))
dim(errorLayer3)
dim(t(thetasSecondLayer))
dim(errorLayer3)
dim(thetasSecondLayer)
dim(errorLayer3)
rm(list=ls())
hypothesis <- function(x, theta) {
z <- x %*% t(theta)
1/(1+exp(-z))
}
addBiasTherm <- function(x) {
cbind(rep(1, nrow(x)), x)
}
cost <- function(x, y, theta) {
m <- nrow(x)
hipo <- hypothesis(x, theta)
vectorCostForEachPoint <- -y*log(hipo) - (1-y)*log(1-hipo)
1/m * sum(vectorCostForEachPoint)
}
input <- rbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input <- addBiasTherm(input)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
thetasSecondLayer <- rbind(c(10, -20, -20))
outputLayerOne <- hypothesis(input, thetasFirstLayer)
inputLayerTwo <- addBiasTherm(outputLayerOne)
output <- hypothesis(inputLayerTwo, thetasSecondLayer)
output
y
output
ouput > 0.9
output > 0.9
y = rbind(1, 0, 0, 0)
errorLayer4 <- output - y
dim(t(thetasSecondLayer))
dim(errorLayer4)
dim(thetasSecondLayer)
dim(t(thetasSecondLayer))
dim(errorLayer4)
y = rbind(1, 0, 0, 0)
dim(y)
errorLayer4 <- output - y
errorLayer4
dim(errorLayer4)
errorLayer4 <- t(errorLayer4)
errorLayer4
dim(errorLayer4)
input <- rbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input <- addBiasTherm(input)
dim(input)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
thetasSecondLayer <- rbind(c(10, -20, -20))
outputLayerOne <- hypothesis(input, thetasFirstLayer)
inputLayerTwo <- addBiasTherm(outputLayerOne)
inputLayerTwo
dim(inputLayerTwo)
thetasSecondLayer <- rbind(c(10, -20, -20))
dim(thetasSecondLayer)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
dim(thetasFirstLayer)
rm(list=ls())
#rm(list = ls())
hypothesis <- function(x, theta) {
z <- x %*% t(theta)
1/(1+exp(-z))
}
addBiasTherm <- function(x) {
cbind(rep(1, nrow(x)), x)
}
cost <- function(x, y, theta) {
m <- nrow(x)
hipo <- hypothesis(x, theta)
vectorCostForEachPoint <- -y*log(hipo) - (1-y)*log(1-hipo)
1/m * sum(vectorCostForEachPoint)
}
input <- rbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input <- addBiasTherm(input)
dim(input)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
dim(thetasFirstLayer)
thetasSecondLayer <- rbind(c(10, -20, -20))
dim(thetasSecondLayer)
## propagation for the first layer
outputLayerOne <- hypothesis(input, thetasFirstLayer)
## propagation for the second layer
inputLayerTwo <- addBiasTherm(outputLayerOne)
dim(inputLayerTwo)
output <- hypothesis(inputLayerTwo, thetasSecondLayer)
dim(outpu)
dim(output)
rm(list=ls())
source('~/github-machine-learning-coursera/neural-network-recomecando/basefunctions.R')
input <- rbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input <- addBiasTherm(input)
dim(input)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
dim(thetasFirstLayer)
thetasSecondLayer <- rbind(c(10, -20, -20))
dim(thetasSecondLayer)
## propagation for the first layer
outputLayerOne <- hypothesis(input, thetasFirstLayer)
## propagation for the second layer
inputLayerTwo <- addBiasTherm(outputLayerOne)
dim(input)
dim(inputLayerTwo)
dim(output)
output <- hypothesis(inputLayerTwo, thetasSecondLayer)
dim(output)
rm(list=ls())
input <- rbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input <- addBiasTherm(input)
source('~/github-machine-learning-coursera/neural-network-recomecando/basefunctions.R')
input <- addBiasTherm(input)
dim(input)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
dim(thetasFirstLayer)
z <- input %*% t(thetasFirstLayer)
dim(z)
source('~/github-machine-learning-coursera/neural-network-recomecando/basefunctions.R')
rm(list=ls())
input <- cbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input
input <- addBiasTherm(input)
source('~/github-machine-learning-coursera/neural-network-recomecando/basefunctions.R')
input <- addBiasTherm(input)
input
dim(input)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
dim
dim(thetasFirstLayer)
theta1 <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
dim(theta1)
theta2 <- rbind(c(10, -20, -20))
dim(theta2)
a1 = x
a2 = hypothesis(a1, theta1)
a1 = x
a2 = hypothesis(a1, theta1)
a1
a1 = x
a1 = input
a1
a2 = hypothesis(a1, theta1)
source('~/github-machine-learning-coursera/neural-network-recomecando/basefunctions.R')
a2 = hypothesis(a1, theta1)
a2 = hypothesis(theta1, a1)
a2
a2 = addBiasTherm(hypothesis(theta1, a1))
dim(a2)
a2
a3 = addBiasTherm(hypothesis(theta2, a2))
a3
a3 = hypothesis(theta2, a2)
a3
a3 > 0.9
y = cbind(c(1, 0, 0, 0))
y
y = rbind(c(1, 0, 0, 0))
y
output = rbind(c(1, 0, 0, 0))
y = output
error3 = a3 - y
error3 = a3 - y
error3
error2 = t(theta2)
t(theta2)
error2 = t(theta2) %*% error3 * (a2 * (1-a2))
error2
theta1
theta1
for(i in 1:nrow(theta1)) {
for(j in 1:ncol(theta1)) {
print(theta[i, j])
}
}
for(i in 1:nrow(theta1)) {
for(j in 1:ncol(theta1)) {
print(theta1[i, j])
}
}
for(i in 1:nrow(theta1)) {
for(j in 1:ncol(theta1)) {
print(a1[,j] * error2[i, ])
}
}
error2
for(i in 1:nrow(theta1)) {
for(j in 1:ncol(theta1)) {
print(a1[,j] * error2[i, ])
}
}
a1[,1]
error2[1,]
a1[1,]
error2[,1]
a1
error2
a1
a[,2]
a1[,2]
error[2,]
error2[2,]
c(4, 5) * c(2, -1)
c(4, 5) * c(2, -1, 0)
a1
error2
a1[,3]
error2[3,]
error2
a2[,1]
error3[1,]
22790/(22790+57737+7003)
1-(22790/(22790+57737+7003))
costFunction <- function(result, expectedResult) {
total <- 0
for (i in 1:ncol(result)) {
total <- total + (     1/m * sum(  -yMatrix[,i]*log(a3[,i]) - (1-yMatrix[,i])*log(1-a3[,i])   )    )
}
total
}
costFunction(a3, yMatrix)
costFunction <- function(result, expectedResult) {
m <- nrow(result)
total <- 0
for (i in 1:ncol(result)) {
total <- total + (     1/m * sum(  -yMatrix[,i]*log(a3[,i]) - (1-yMatrix[,i])*log(1-a3[,i])   )    )
}
total
}
costFunction(a3, yMatrix)
costFunction <- function(result, y) {
m <- nrow(result)
total <- 0
for (i in 1:ncol(result)) {
total <- total + (     1/m * sum(  -y[,i]*log(result[,i]) - (1-y[,i])*log(1-result[,i])   )    )
}
total
}
costFunction(a3, yMatrix)
yMatrix <- createResultMatrix(y)
createResultMatrix <- function(y) {
yMatrix <- matrix(NA, nrow=length(y), ncol=10)
for(i in 1:length(y)) {
pointVector <- rep(0, 10)
index <- y[i]
pointVector[index] <- 1
yMatrix[i,] <- pointVector
}
yMatrix
}
yMatrix <- createResultMatrix(y)
costFunction(a3, yMatrix)
costFunction <- function(result, y) {
m <- nrow(result)
total <- 0
for (i in 1:ncol(result)) {
total <- total + (     1/m * sum(  -y[,i]*log(result[,i]) - (1-y[,i])*log(1-result[,i])   )    )
}
total
}
a3 <- hypothesis(theta2, a2)
yMatrix <- createResultMatrix(y)
costFunction(a3, y)
costFunction(a3, yMatrix)
dim(a3)
rm(list = ls())
### classical approach, that uses each row for a given entry point
addBiasThermFirstColumn <- function(x) {
cbind(rep(1, nrow(x)), x)
}
hypothesis <- function(theta, x) {
z <- x %*% t(theta)
1/(1+exp(-z))
}
createResultMatrix <- function(y) {
yMatrix <- matrix(NA, nrow=length(y), ncol=10)
for(i in 1:length(y)) {
pointVector <- rep(0, 10)
index <- y[i]
pointVector[index] <- 1
yMatrix[i,] <- pointVector
}
yMatrix
}
costFunction <- function(result, y) {
m <- nrow(result)
total <- 0
for (i in 1:ncol(result)) {
total <- total + (     1/m * sum(  -y[,i]*log(result[,i]) - (1-y[,i])*log(1-result[,i])   )    )
}
total
}
#### bizarre
addBiasThermFirstRow <- function(x) {
rbind(rep(1, ncol(x)), x)
}
hypothesisBizarre <- function(theta, x) {
z <- theta %*% x
1/(1+exp(-z))
}
createResultMatrixBizarre <- function(y) {
yMatrix <- matrix(NA, nrow=10, ncol=length(y))
for(i in 1:length(y)) {
pointVector <- rep(0, 10)
index <- y[i]
pointVector[index] <- 1
yMatrix[,i] <- pointVector
}
yMatrix
}
# cost function defined at 2.2.1
legacyCost <- function(x, y, theta) {
m <- nrow(x)
hipo <- hypothesis(theta, x)
vectorCostForEachPoint <-
1/m * sum(vectorCostForEachPoint)
}
setwd('/home/nozes/github-machine-learning-coursera/exercise4')
x = read.csv2('input/data1.txt', sep=',', stringsAsFactors = FALSE, header = FALSE)
x <- as.matrix(sapply(x, as.numeric))
y = read.csv2('input/y.txt', sep=',', stringsAsFactors = FALSE, header = FALSE)$V1
y <- as.matrix(sapply(y, as.numeric))
theta1 = read.csv2('input/theta1.txt', sep=',', stringsAsFactors = FALSE, header = FALSE)
theta2 = read.csv2('input/theta2.txt', sep=',', stringsAsFactors = FALSE, header = FALSE)
theta1 <- as.matrix(sapply(theta1, as.numeric))
theta2 <- as.matrix(sapply(theta2, as.numeric))
a1 <- addBiasThermFirstColumn(x)
a2 <- addBiasThermFirstColumn(hypothesis(theta1, a1))
a3 <- hypothesis(theta2, a2)
yMatrix <- createResultMatrix(y)
dim(a3)
dim(yMatrix)
costFunction(a3, yMatrix)
costFunction <- function(result, y) {
m <- nrow(result)
soma <- sum(-y*log(result) - (1-y)*log(1-result))
total <-  1/m * sum( soma    )
total
}
costFunction(a3, yMatrix)
costFunction(a3, yMatrix)
rm(list = ls())
addBiasThermFirstColumn <- function(x) {
cbind(rep(1, nrow(x)), x)
}
hypothesis <- function(theta, x) {
z <- x %*% t(theta)
1/(1+exp(-z))
}
createResultMatrix <- function(y) {
yMatrix <- matrix(NA, nrow=length(y), ncol=10)
for(i in 1:length(y)) {
pointVector <- rep(0, 10)
index <- y[i]
pointVector[index] <- 1
yMatrix[i,] <- pointVector
}
yMatrix
}
costFunction <- function(result, y) {
m <- nrow(result)
soma <- sum(-y*log(result) - (1-y)*log(1-result))
total <-  1/m * sum( soma    )
total
}
setwd('/home/nozes/github-machine-learning-coursera/exercise4')
x = read.csv2('input/data1.txt', sep=',', stringsAsFactors = FALSE, header = FALSE)
x <- as.matrix(sapply(x, as.numeric))
y = read.csv2('input/y.txt', sep=',', stringsAsFactors = FALSE, header = FALSE)$V1
y <- as.matrix(sapply(y, as.numeric))
theta1 = read.csv2('input/theta1.txt', sep=',', stringsAsFactors = FALSE, header = FALSE)
theta2 = read.csv2('input/theta2.txt', sep=',', stringsAsFactors = FALSE, header = FALSE)
theta1 <- as.matrix(sapply(theta1, as.numeric))
theta2 <- as.matrix(sapply(theta2, as.numeric))
a1 <- addBiasThermFirstColumn(x)
a2 <- addBiasThermFirstColumn(hypothesis(theta1, a1))
a3 <- hypothesis(theta2, a2)
yMatrix <- createResultMatrix(y)
dim(a3)
dim(yMatrix)
costFunction(a3, yMatrix)
rm(list = ls())
### classical approach, that uses each row for a given entry point
addBiasThermFirstColumn <- function(x) {
cbind(rep(1, nrow(x)), x)
}
hypothesis <- function(theta, x) {
z <- x %*% t(theta)
1/(1+exp(-z))
}
createResultMatrix <- function(y) {
yMatrix <- matrix(NA, nrow=length(y), ncol=10)
for(i in 1:length(y)) {
pointVector <- rep(0, 10)
index <- y[i]
pointVector[index] <- 1
yMatrix[i,] <- pointVector
}
yMatrix
}
costFunction <- function(result, y) {
m <- nrow(result)
soma <- sum(-y*log(result) - (1-y)*log(1-result))
total <-  1/m * sum( soma    )
total
}
setwd('/home/nozes/github-machine-learning-coursera/exercise4')
x = read.csv2('input/data1.txt', sep=',', stringsAsFactors = FALSE, header = FALSE)
x <- as.matrix(sapply(x, as.numeric))
y = read.csv2('input/y.txt', sep=',', stringsAsFactors = FALSE, header = FALSE)$V1
y <- as.matrix(sapply(y, as.numeric))
theta1 = read.csv2('input/theta1.txt', sep=',', stringsAsFactors = FALSE, header = FALSE)
theta2 = read.csv2('input/theta2.txt', sep=',', stringsAsFactors = FALSE, header = FALSE)
theta1 <- as.matrix(sapply(theta1, as.numeric))
theta2 <- as.matrix(sapply(theta2, as.numeric))
a1 <- addBiasThermFirstColumn(x)
a2 <- addBiasThermFirstColumn(hypothesis(theta1, a1))
a3 <- hypothesis(theta2, a2)
yMatrix <- createResultMatrix(y)
dim(a3)
dim(yMatrix)
costFunction(a3, yMatrix)
dim(theta1)
dim(theta1)
theta1[1,]
theta1[,1]
dim(theta1)
theta1[,2:401]
dim(theta1[,2:401])
dim(theta2)
dim(theta2[,2:26])
costFunctionWithRegularization <- function(result, y, theta1, theta2, lambda) {
m <- nrow(result)
soma <- sum(-y*log(result) - (1-y)*log(1-result))
newTheta1 <- theta1[,2:401]
newTheta2 <- theta2[,2:26]
regularization <- lambda/(2*m)  * (sum(newTheta1^2) + sum(newTheta2^2))
total <-  1/m * sum( soma    )
total
}
costFunctionWithRegularization(a3, yMatrix, theta1, theta2, 1)
costFunctionWithRegularization <- function(result, y, theta1, theta2, lambda) {
m <- nrow(result)
soma <- sum(-y*log(result) - (1-y)*log(1-result))
newTheta1 <- theta1[,2:401]
newTheta2 <- theta2[,2:26]
regularization <- lambda/(2*m)  * (sum(newTheta1^2) + sum(newTheta2^2))
total <-  1/m * sum( soma    )   + regularization
total
}
costFunctionWithRegularization(a3, yMatrix, theta1, theta2, 1)
