)
input <- addBiasTherm(input)
input
result <- hypothesis(inputWithBiasTherm, thetas)
result <- hypothesis(input, thetas)
result > 0.9
result < 0.1
result <- hypothesis(input, thetas)
result > 0.9
input
result <- hypothesis(input, thetas)
result > 0.9
result < 0.1
thetas <- rbind(c(10, 20, 20))
input <- rbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input <- addBiasTherm(input)
result <- hypothesis(input, thetas)
result > 0.9
result < 0.1
thetas <- rbind(c(-10, 20, 20))
input <- rbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input <- addBiasTherm(input)
result <- hypothesis(input, thetas)
result > 0.9
result < 0.1
thetas <- rbind(c(10, -20, -20))
input <- rbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input <- addBiasTherm(input)
result <- hypothesis(input, thetas)
result > 0.9
input <- rbind(
rm(list = ls())
input <- rbind(
c(0, 0),
c(0, 1),
c(1, 0),
rm(list = ls())
input <- rbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input <- addBiasTherm(input)
source('~/github-machine-learning-coursera/neural-network-recomecando/basefunctions.R')
input <- addBiasTherm(input)
input
thetas <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
thetas
thetas <- rbind(c(10, -20, -20))
input
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
thetasSecondLayer <- rbind(c(10, -20, -20))
hypothesis(input, thetasFirstLayer)
outputLayerOne <- hypothesis(input, thetasFirstLayer)
outputLayerOne
ouputLayerOne > 0.9
outputLayerOne > 0.9
outputLayerOne < 0.1
inputLayerTwo <- addBiasTherm(outputLayerOne)
inputLayerTwo
inputLayerTwo <- addBiasTherm(outputLayerOne)
inputLayerTwo
output <- hypothesis(inputLayerTwo, thetasSecondLayer)
output
output > 0.9
source('~/github-machine-learning-coursera/neural-network-recomecando/basefunctions.R')
cost <- function(x, y, theta) {
m <- nrow(x)
hipo <- hypothesis(x, theta)
vectorCostForEachPoint <- -y*log(hipo) - (1-y)*log(1-hipo)
1/m * sum(vectorCostForEachPoint)
}
source('~/github-machine-learning-coursera/neural-network-recomecando/basefunctions.R')
source('~/github-machine-learning-coursera/neural-network-recomecando/basefunctions.R')
inputLayerTwo
output
output>0.9
y <- rbind(c(1, 0, 0, 0))
y
cost(inputLayerTwo, y, thetasSecondLayer)
m <- nrow(x)
m <- nrow(inputLayerTwo)
m
inputLayerTwo
thetasSecondLayer
hipo <- hypothesis(inputLayerTwo, thetasSecondLayer)
hipo
dim(y)
dim(hipo)
vectorCostForEachPoint <- -y*log(hipo) - (1-y)*log(1-hipo)
hipo
-y
y <- rbind(1, 0, 0, 0)
o
y
vectorCostForEachPoint <- -y*log(hipo) - (1-y)*log(1-hipo)
vectorCostForEachPoint
vectorCostForEachPoint > 0.9
vectorCostForEachPoint < 0.1
sum(vectorCostForEachPoint)
cost(inputLayerTwo, y, thetasSecondLayer)
cost <- cost(inputLayerTwo, y, thetasSecondLayer)
cost <- 0.1
cost <- cost(inputLayerTwo, y, thetasSecondLayer)
cost < 0.1
cost < 1
cost < 0.9
cost < 0.5
cost < 0.4
cost < 0.2
cost < 0.1
y <- rbind(1, 0, 0, 1)
cost <- cost(inputLayerTwo, y, thetasSecondLayer)
y <- rbind(1, 0, 0, 1)
cost <- cost(inputLayerTwo, y, thetasSecondLayer)
source('~/github-machine-learning-coursera/neural-network-recomecando/basefunctions.R')
cost <- cost(inputLayerTwo, y, thetasSecondLayer)
cost
cost <- cost(inputLayerTwo, y, thetasSecondLayer)
source('~/github-machine-learning-coursera/neural-network-recomecando/basefunctions.R')
input <- rbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input <- addBiasTherm(input)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
thetasSecondLayer <- rbind(c(10, -20, -20))
## propagation for the first layer
outputLayerOne <- hypothesis(input, thetasFirstLayer)
## propagation for the second layer
inputLayerTwo <- addBiasTherm(outputLayerOne)
output <- hypothesis(inputLayerTwo, thetasSecondLayer)
cost <- cost(inputLayerTwo, y, thetasSecondLayer)
print(cost)
output
y
y
errorLayer3 <- output - y
errorLayer3
thetasFirstLayer
errorLayer2 <- t(thetasFirstLayer)
t(thetasFirstLayer)
dim(t(thetasFirstLayer))
dim(errorLayer3)
dim(t(thetasSecondLayer))
dim(errorLayer3)
dim(thetasSecondLayer)
dim(errorLayer3)
rm(list=ls())
hypothesis <- function(x, theta) {
z <- x %*% t(theta)
1/(1+exp(-z))
}
addBiasTherm <- function(x) {
cbind(rep(1, nrow(x)), x)
}
cost <- function(x, y, theta) {
m <- nrow(x)
hipo <- hypothesis(x, theta)
vectorCostForEachPoint <- -y*log(hipo) - (1-y)*log(1-hipo)
1/m * sum(vectorCostForEachPoint)
}
input <- rbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input <- addBiasTherm(input)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
thetasSecondLayer <- rbind(c(10, -20, -20))
outputLayerOne <- hypothesis(input, thetasFirstLayer)
inputLayerTwo <- addBiasTherm(outputLayerOne)
output <- hypothesis(inputLayerTwo, thetasSecondLayer)
output
y
output
ouput > 0.9
output > 0.9
y = rbind(1, 0, 0, 0)
errorLayer4 <- output - y
dim(t(thetasSecondLayer))
dim(errorLayer4)
dim(thetasSecondLayer)
dim(t(thetasSecondLayer))
dim(errorLayer4)
y = rbind(1, 0, 0, 0)
dim(y)
errorLayer4 <- output - y
errorLayer4
dim(errorLayer4)
errorLayer4 <- t(errorLayer4)
errorLayer4
dim(errorLayer4)
input <- rbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input <- addBiasTherm(input)
dim(input)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
thetasSecondLayer <- rbind(c(10, -20, -20))
outputLayerOne <- hypothesis(input, thetasFirstLayer)
inputLayerTwo <- addBiasTherm(outputLayerOne)
inputLayerTwo
dim(inputLayerTwo)
thetasSecondLayer <- rbind(c(10, -20, -20))
dim(thetasSecondLayer)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
dim(thetasFirstLayer)
rm(list=ls())
#rm(list = ls())
hypothesis <- function(x, theta) {
z <- x %*% t(theta)
1/(1+exp(-z))
}
addBiasTherm <- function(x) {
cbind(rep(1, nrow(x)), x)
}
cost <- function(x, y, theta) {
m <- nrow(x)
hipo <- hypothesis(x, theta)
vectorCostForEachPoint <- -y*log(hipo) - (1-y)*log(1-hipo)
1/m * sum(vectorCostForEachPoint)
}
input <- rbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input <- addBiasTherm(input)
dim(input)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
dim(thetasFirstLayer)
thetasSecondLayer <- rbind(c(10, -20, -20))
dim(thetasSecondLayer)
## propagation for the first layer
outputLayerOne <- hypothesis(input, thetasFirstLayer)
## propagation for the second layer
inputLayerTwo <- addBiasTherm(outputLayerOne)
dim(inputLayerTwo)
output <- hypothesis(inputLayerTwo, thetasSecondLayer)
dim(outpu)
dim(output)
rm(list=ls())
source('~/github-machine-learning-coursera/neural-network-recomecando/basefunctions.R')
input <- rbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input <- addBiasTherm(input)
dim(input)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
dim(thetasFirstLayer)
thetasSecondLayer <- rbind(c(10, -20, -20))
dim(thetasSecondLayer)
## propagation for the first layer
outputLayerOne <- hypothesis(input, thetasFirstLayer)
## propagation for the second layer
inputLayerTwo <- addBiasTherm(outputLayerOne)
dim(input)
dim(inputLayerTwo)
dim(output)
output <- hypothesis(inputLayerTwo, thetasSecondLayer)
dim(output)
rm(list=ls())
input <- rbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input <- addBiasTherm(input)
source('~/github-machine-learning-coursera/neural-network-recomecando/basefunctions.R')
input <- addBiasTherm(input)
dim(input)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
dim(thetasFirstLayer)
z <- input %*% t(thetasFirstLayer)
dim(z)
source('~/github-machine-learning-coursera/neural-network-recomecando/basefunctions.R')
rm(list=ls())
input <- cbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input
input <- addBiasTherm(input)
source('~/github-machine-learning-coursera/neural-network-recomecando/basefunctions.R')
input <- addBiasTherm(input)
input
dim(input)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
dim
dim(thetasFirstLayer)
theta1 <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
dim(theta1)
theta2 <- rbind(c(10, -20, -20))
dim(theta2)
a1 = x
a2 = hypothesis(a1, theta1)
a1 = x
a2 = hypothesis(a1, theta1)
a1
a1 = x
a1 = input
a1
a2 = hypothesis(a1, theta1)
source('~/github-machine-learning-coursera/neural-network-recomecando/basefunctions.R')
a2 = hypothesis(a1, theta1)
a2 = hypothesis(theta1, a1)
a2
a2 = addBiasTherm(hypothesis(theta1, a1))
dim(a2)
a2
a3 = addBiasTherm(hypothesis(theta2, a2))
a3
a3 = hypothesis(theta2, a2)
a3
a3 > 0.9
y = cbind(c(1, 0, 0, 0))
y
y = rbind(c(1, 0, 0, 0))
y
output = rbind(c(1, 0, 0, 0))
y = output
error3 = a3 - y
error3 = a3 - y
error3
error2 = t(theta2)
t(theta2)
error2 = t(theta2) %*% error3 * (a2 * (1-a2))
error2
theta1
theta1
for(i in 1:nrow(theta1)) {
for(j in 1:ncol(theta1)) {
print(theta[i, j])
}
}
for(i in 1:nrow(theta1)) {
for(j in 1:ncol(theta1)) {
print(theta1[i, j])
}
}
for(i in 1:nrow(theta1)) {
for(j in 1:ncol(theta1)) {
print(a1[,j] * error2[i, ])
}
}
error2
for(i in 1:nrow(theta1)) {
for(j in 1:ncol(theta1)) {
print(a1[,j] * error2[i, ])
}
}
a1[,1]
error2[1,]
a1[1,]
error2[,1]
a1
error2
a1
a[,2]
a1[,2]
error[2,]
error2[2,]
c(4, 5) * c(2, -1)
c(4, 5) * c(2, -1, 0)
a1
error2
a1[,3]
error2[3,]
error2
a2[,1]
error3[1,]
22790/(22790+57737+7003)
1-(22790/(22790+57737+7003))
0.915^2
1-0.12
0.88^4
1-0.88^4
24/112
24/625
100-7
0.93/0.05
0.07/0.05
0.03/0.95
0.97/0.95
0.93*0.05
0.07*0.05
0.03*0.95
0.97*0.95
0.9215+0.0285+0.0035+0.0465
0.0465/(0.0465+0.0285)
24/46
svm.model <- svm(y ~ x, cost = 1)
library(e1071)
library(rpart)
data(Glass, package="mlbench")
glass <- Glass
index <- 1:nrow(glass)
testIndex <- sample(index, trunc(length(index)/3))
testset <- glass[testIndex,]
trainset <- glass[-testIndex,]
svm.model <- svm(y ~ x, cost = 1)
svm.pred <- predict(svm.model, x)
library(e1071)
library(rpart)
data(Glass, package="mlbench")
glass <- Glass
index <- 1:nrow(glass)
testIndex <- sample(index, trunc(length(index)/3))
testset <- glass[testIndex,]
trainset <- glass[-testIndex,]
svm.model <- svm(y ~ x, cost = 1)
setwd('/home/nozes/github-machine-learning-coursera/exercise6')
x <- read.csv2('input/xis.txt', sep = ' ' , header = FALSE, stringsAsFactors = FALSE)
x <- cbind(as.numeric(as.character(x[,1])), as.numeric(as.character(x[,2])))
y <- read.csv2('input/ipsilone.txt', sep = ' ' , header = FALSE, stringsAsFactors = TRUE)
y <- as.factor(y[,1])
svm.model <- svm(y ~ x, cost = 1)
svm.pred <- predict(svm.model, x)
svm.pred
svm.model <- svm(y ~ x, cost = 0.1)
svm.pred <- predict(svm.model, x)
svm.pred
svm.model <- svm(y ~ x, cost = 1)
svm.pred <- predict(svm.model, x)
svm.pred
colors = sapply(y, function(x) {
if(x == 0) {
return(2)
} else {
return(4)
}
})
plot(x, col = colors, pch = 19)
library(e1071)
library(rpart)
data(Glass, package="mlbench")
glass <- Glass
index <- 1:nrow(glass)
testIndex <- sample(index, trunc(length(index)/3))
testset <- glass[testIndex,]
trainset <- glass[-testIndex,]
svm.model <- svm(y ~ x, cost = 1)
svm.pred <- predict(svm.model, x)
svm.model <- svm(y ~ x, cost = 1)
svm.pred <- predict(svm.model, x)
svm.pred
svm.model <- svm(y ~ x, cost = 0.1)
svm.pred <- predict(svm.model, x)
svm.pred
