cost < 0.9
cost < 0.5
cost < 0.4
cost < 0.2
cost < 0.1
y <- rbind(1, 0, 0, 1)
cost <- cost(inputLayerTwo, y, thetasSecondLayer)
y <- rbind(1, 0, 0, 1)
cost <- cost(inputLayerTwo, y, thetasSecondLayer)
source('~/github-machine-learning-coursera/neural-network-recomecando/basefunctions.R')
cost <- cost(inputLayerTwo, y, thetasSecondLayer)
cost
cost <- cost(inputLayerTwo, y, thetasSecondLayer)
source('~/github-machine-learning-coursera/neural-network-recomecando/basefunctions.R')
input <- rbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input <- addBiasTherm(input)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
thetasSecondLayer <- rbind(c(10, -20, -20))
## propagation for the first layer
outputLayerOne <- hypothesis(input, thetasFirstLayer)
## propagation for the second layer
inputLayerTwo <- addBiasTherm(outputLayerOne)
output <- hypothesis(inputLayerTwo, thetasSecondLayer)
cost <- cost(inputLayerTwo, y, thetasSecondLayer)
print(cost)
output
y
y
errorLayer3 <- output - y
errorLayer3
thetasFirstLayer
errorLayer2 <- t(thetasFirstLayer)
t(thetasFirstLayer)
dim(t(thetasFirstLayer))
dim(errorLayer3)
dim(t(thetasSecondLayer))
dim(errorLayer3)
dim(thetasSecondLayer)
dim(errorLayer3)
rm(list=ls())
hypothesis <- function(x, theta) {
z <- x %*% t(theta)
1/(1+exp(-z))
}
addBiasTherm <- function(x) {
cbind(rep(1, nrow(x)), x)
}
cost <- function(x, y, theta) {
m <- nrow(x)
hipo <- hypothesis(x, theta)
vectorCostForEachPoint <- -y*log(hipo) - (1-y)*log(1-hipo)
1/m * sum(vectorCostForEachPoint)
}
input <- rbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input <- addBiasTherm(input)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
thetasSecondLayer <- rbind(c(10, -20, -20))
outputLayerOne <- hypothesis(input, thetasFirstLayer)
inputLayerTwo <- addBiasTherm(outputLayerOne)
output <- hypothesis(inputLayerTwo, thetasSecondLayer)
output
y
output
ouput > 0.9
output > 0.9
y = rbind(1, 0, 0, 0)
errorLayer4 <- output - y
dim(t(thetasSecondLayer))
dim(errorLayer4)
dim(thetasSecondLayer)
dim(t(thetasSecondLayer))
dim(errorLayer4)
y = rbind(1, 0, 0, 0)
dim(y)
errorLayer4 <- output - y
errorLayer4
dim(errorLayer4)
errorLayer4 <- t(errorLayer4)
errorLayer4
dim(errorLayer4)
input <- rbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input <- addBiasTherm(input)
dim(input)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
thetasSecondLayer <- rbind(c(10, -20, -20))
outputLayerOne <- hypothesis(input, thetasFirstLayer)
inputLayerTwo <- addBiasTherm(outputLayerOne)
inputLayerTwo
dim(inputLayerTwo)
thetasSecondLayer <- rbind(c(10, -20, -20))
dim(thetasSecondLayer)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
dim(thetasFirstLayer)
rm(list=ls())
#rm(list = ls())
hypothesis <- function(x, theta) {
z <- x %*% t(theta)
1/(1+exp(-z))
}
addBiasTherm <- function(x) {
cbind(rep(1, nrow(x)), x)
}
cost <- function(x, y, theta) {
m <- nrow(x)
hipo <- hypothesis(x, theta)
vectorCostForEachPoint <- -y*log(hipo) - (1-y)*log(1-hipo)
1/m * sum(vectorCostForEachPoint)
}
input <- rbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input <- addBiasTherm(input)
dim(input)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
dim(thetasFirstLayer)
thetasSecondLayer <- rbind(c(10, -20, -20))
dim(thetasSecondLayer)
## propagation for the first layer
outputLayerOne <- hypothesis(input, thetasFirstLayer)
## propagation for the second layer
inputLayerTwo <- addBiasTherm(outputLayerOne)
dim(inputLayerTwo)
output <- hypothesis(inputLayerTwo, thetasSecondLayer)
dim(outpu)
dim(output)
rm(list=ls())
source('~/github-machine-learning-coursera/neural-network-recomecando/basefunctions.R')
input <- rbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input <- addBiasTherm(input)
dim(input)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
dim(thetasFirstLayer)
thetasSecondLayer <- rbind(c(10, -20, -20))
dim(thetasSecondLayer)
## propagation for the first layer
outputLayerOne <- hypothesis(input, thetasFirstLayer)
## propagation for the second layer
inputLayerTwo <- addBiasTherm(outputLayerOne)
dim(input)
dim(inputLayerTwo)
dim(output)
output <- hypothesis(inputLayerTwo, thetasSecondLayer)
dim(output)
rm(list=ls())
input <- rbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input <- addBiasTherm(input)
source('~/github-machine-learning-coursera/neural-network-recomecando/basefunctions.R')
input <- addBiasTherm(input)
dim(input)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
dim(thetasFirstLayer)
z <- input %*% t(thetasFirstLayer)
dim(z)
source('~/github-machine-learning-coursera/neural-network-recomecando/basefunctions.R')
rm(list=ls())
input <- cbind(
c(0, 0),
c(0, 1),
c(1, 0),
c(1, 1)
)
input
input <- addBiasTherm(input)
source('~/github-machine-learning-coursera/neural-network-recomecando/basefunctions.R')
input <- addBiasTherm(input)
input
dim(input)
thetasFirstLayer <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
dim
dim(thetasFirstLayer)
theta1 <- rbind(
c(-30, 20, 20),
c(-10, 20, 20)
)
dim(theta1)
theta2 <- rbind(c(10, -20, -20))
dim(theta2)
a1 = x
a2 = hypothesis(a1, theta1)
a1 = x
a2 = hypothesis(a1, theta1)
a1
a1 = x
a1 = input
a1
a2 = hypothesis(a1, theta1)
source('~/github-machine-learning-coursera/neural-network-recomecando/basefunctions.R')
a2 = hypothesis(a1, theta1)
a2 = hypothesis(theta1, a1)
a2
a2 = addBiasTherm(hypothesis(theta1, a1))
dim(a2)
a2
a3 = addBiasTherm(hypothesis(theta2, a2))
a3
a3 = hypothesis(theta2, a2)
a3
a3 > 0.9
y = cbind(c(1, 0, 0, 0))
y
y = rbind(c(1, 0, 0, 0))
y
output = rbind(c(1, 0, 0, 0))
y = output
error3 = a3 - y
error3 = a3 - y
error3
error2 = t(theta2)
t(theta2)
error2 = t(theta2) %*% error3 * (a2 * (1-a2))
error2
theta1
theta1
for(i in 1:nrow(theta1)) {
for(j in 1:ncol(theta1)) {
print(theta[i, j])
}
}
for(i in 1:nrow(theta1)) {
for(j in 1:ncol(theta1)) {
print(theta1[i, j])
}
}
for(i in 1:nrow(theta1)) {
for(j in 1:ncol(theta1)) {
print(a1[,j] * error2[i, ])
}
}
error2
for(i in 1:nrow(theta1)) {
for(j in 1:ncol(theta1)) {
print(a1[,j] * error2[i, ])
}
}
a1[,1]
error2[1,]
a1[1,]
error2[,1]
a1
error2
a1
a[,2]
a1[,2]
error[2,]
error2[2,]
c(4, 5) * c(2, -1)
c(4, 5) * c(2, -1, 0)
a1
error2
a1[,3]
error2[3,]
error2
a2[,1]
error3[1,]
22790/(22790+57737+7003)
1-(22790/(22790+57737+7003))
0.915^2
1-0.12
0.88^4
1-0.88^4
24/112
24/625
100-7
0.93/0.05
0.07/0.05
0.03/0.95
0.97/0.95
0.93*0.05
0.07*0.05
0.03*0.95
0.97*0.95
0.9215+0.0285+0.0035+0.0465
0.0465/(0.0465+0.0285)
24/46
rm(list = ls())
n <- 150 # number of data points
p <- 2 # dimension
sigma <- 1 # variance of the distribution
meanpos <- 0 # centre of the distribution of positive examples
meanneg <- 3 # centre of the distribution of negative examples
npos <- round(n/2) # number of positive examples
nneg <- n-npos # number of negative examples
# Generate the positive and negative examples
xpos <- matrix(rnorm(npos*p,mean=meanpos,sd=sigma),npos,p)
xneg <- matrix(rnorm(nneg*p,mean=meanneg,sd=sigma),npos,p)
x <- rbind(xpos,xneg)
# Generate the labels
y <- matrix(c(rep(1,npos),rep(-1,nneg)))
# Visualize the data
plot(x,col=ifelse(y>0,1,2))
legend("topleft",c(’Positive’,’Negative’),col=seq(2),pch=1,text.col=seq(2))
ntrain <- round(n*0.8) # number of training examples
tindex <- sample(n,ntrain) # indices of training samples
xtrain <- x[tindex,]
xtest <- x[-tindex,]
ytrain <- y[tindex]
ytest <- y[-tindex]
istrain=rep(0,n)
istrain[tindex]=1
plot(x,col=ifelse(y>0,1,2),pch=ifelse(istrain==1,1,2))
ntrain <- round(n*0.8) # number of training examples
tindex <- sample(n,ntrain) # indices of training samples
xtrain <- x[tindex,]
xtest <- x[-tindex,]
ytrain <- y[tindex]
ytest <- y[-tindex]
istrain=rep(0,n)
istrain[tindex]=1
# Visualize
plot(x,col=ifelse(y>0,1,2),pch=ifelse(istrain==1,1,2))
legend("topleft",c(’Positive Train’,’Positive Test’,’Negative Train’,’Negative Test’),
col=c(1,1,2,2),pch=c(1,2,1,2),text.col=c(1,1,2,2))
library(kernlab)
install.packages(kernlab)
library(kernlab)
install.packages('kernlab')
library(kernlab)
library(kernlab)
install.packages('kernlab')
install.packages('kernlab')
install.packages('kernlab')
library(kernlab)
xtrain
ytrain
svp <- ksvm(xtrain, ytrain, type='C-svc', kernel='vanilladot', C=100, scaled=c())
svp
attributes(svp)
alpha(svp)
plot(svp, data=xtrain)
svp
attributes(svp)
alpha(svp)
alphaindex(svp)
b(svp)
svp
attributes(svp)
alpha(svp)
alphaindex(svp)
b(svp)
alpha(svp)
alphaindex(svp)
plot(svp, data=xtrain)
b(svp)
svp
attributes(svp)
alpha(svp)
?alpha
?alpha
?alphaindex
?alphaindex
alphaindex(svp)
60043+4000+1800
60043+1800+4000+1000
60043+1800+4000+1000+2500
setwd('/home/nozes/github-machine-learning-coursera/exercise8')
rm(list = ls())
setwd('/home/nozes/github-machine-learning-coursera/exercise8')
data1 <- read.csv2('input/data1.txt', sep = ' ', header = FALSE)
data1 <- read.csv2('input/data1X.txt', sep = ' ', header = FALSE)
dim(data1)
data1[1,]
data1[1,-1]
data1 <- data1[,-1]
dim(data1)
data1[1,]
data1 <- as.matrix(data1[,-1])
clasdata1[1,1]
data1 <- as.matrix(data1[,-1])
clasdata1[1,1]
class(data1[1,1])
class(data1[1,1])
data1[1,1]
data1[1,]
data1[1,1]
data1 <- read.csv2('input/data1X.txt', sep = ' ', header = FALSE)
data1 <- as.matrix(data1[,-1])
class(data1[1,1])
class(data1[1,2])
dim(data1)
data1 <- sapply(data1, as.numeric)
class(data1[1,2])
class(data1[1,1])
data1 <- as.matrix(sapply(data1, as.numeric))
data1 <- read.csv2('input/data1X.txt', sep = ' ', header = FALSE)
data1 <- as.matrix(data1[,-1])
data1 <- as.matrix(sapply(data1, as.numeric))
data1
class(data1[1,1])
dim(data1)
data1 = read.csv2('input/data1X.txt', sep=',', stringsAsFactors = FALSE, header = FALSE)
data1 <- as.matrix(sapply(x, as.numeric))
data1 <- data1[,-1]
data1
dim(data1)
data1 = read.csv2('input/data1X.txt', sep=',', stringsAsFactors = FALSE, header = FALSE)
dim(data1)
data1 = read.csv2('input/data1X.txt', sep=',', stringsAsFactors = FALSE, header = FALSE)
dim(data1)
data1 = read.csv2('input/data1X.txt', sep=' ', stringsAsFactors = FALSE, header = FALSE)
data1 <- as.matrix(sapply(x, as.numeric))
data1 <- data1[,-1]
dim(data1)
dim(data1)
data1[1,1]
class(data1[1,1])
data1 <- as.matrix(sapply(x, as.numeric))
data1 <- data1[,-1]
data1 = read.csv2('input/data1X.txt', sep=' ', stringsAsFactors = FALSE, header = FALSE)
data1 <- as.matrix(sapply(x, as.numeric))
data1 = read.csv2('input/data1X.txt', sep=' ', stringsAsFactors = FALSE, header = FALSE)
data1 <- as.matrix(sapply(data1, as.numeric))
data1 <- data1[,-1]
class(data1[1,1])
dim(data1)
plot(data1)
data1[,1]
mean(data1[,1])
sd(data1[,1])
meanX2 <- mean(data1[,2])
sdX2   <- sd(data1[,2])
meanX1 <- mean(data1[,1])
sdX1   <- sd(data1[,1])
pnorm(data1[,1], meanX1, sdX1)
pnorm(data1[,1], meanX1, sdX1) < 0.10
tenPercent <- pnorm(data1[,1], meanX1, sdX1) < 0.10
subset(tenPercent, TRUE)
subset(tenPercent, tenPercent == TRUE)
size(subset(tenPercent, tenPercent == TRUE))
length(subset(tenPercent, tenPercent == TRUE))
plot(data1)
tenPercent <- pnorm(data1[,1], meanX1, sdX1) < 0.100
length(subset(tenPercent, tenPercent == TRUE))
tenPercent <- pnorm(data1[,1], meanX1, sdX1) < 1
length(subset(tenPercent, tenPercent == TRUE))
tenPercent <- pnorm(data1[,1], meanX1, sdX1) < 0.05
length(subset(tenPercent, tenPercent == TRUE))
tenPercent <- pnorm(data1[,2], meanX2, sdX2) < 0.05
length(subset(tenPercent, tenPercent == TRUE))
data1
tenPercent <- pnorm(data1[,1], meanX1, sdX1) < 0.05
tenPercent <- pnorm(data1[,1], meanX1, sdX1)
tenPercent
data1 <- cbind(data1, tenPercent)
data1
tenPercent <- pnorm(data1[,2], meanX2, sdX2) < 0.05
data1 <- cbind(data1, tenPercent)
data1
rm(list = ls())
setwd('/home/nozes/github-machine-learning-coursera/exercise8')
data1 = read.csv2('input/data1X.txt', sep=' ', stringsAsFactors = FALSE, header = FALSE)
data1 <- as.matrix(sapply(data1, as.numeric))
data1 <- data1[,-1]
plot(data1)
meanX1 <- mean(data1[,1])
sdX1   <- sd(data1[,1])
meanX2 <- mean(data1[,2])
sdX2   <- sd(data1[,2])
tenPercent <- pnorm(data1[,1], meanX1, sdX1)
data1 <- cbind(data1, tenPercent)
data1
